# IE643_Project

This project is my first major undertaking in the Machine Learning / Deep Learning field. I will be re-producing the results of a published research paper under the guidance of Professor P. Balamurugan of IIT Bombay as a part of the course IE 643 - Deep Learning Theory and Practice. Although, we were allowed to make teams, to ensure maximum learning, I am doing this project single-handedly.

Research paper link:

https://papers.nips.cc/paper/9109-nonparametric-density-estimation-convergence-rates-for-gans-under-besov-ipm-losses.pdf

I will be borrowing the Abstract of paper to introduce with paper's motives for anybody to read and understand what I am going to do:

We study the problem of estimating a nonparametric probability density under a large family of losses called Besov IPMs, which include, for example, L p distances, total variation distance, and generalizations of both Wasserstein and KolmogorovSmirnov distances. For a wide variety of settings, we provide both lower and upper bounds, identifying precisely how the choice of loss function and assumptions on the data interact to determine the minimax optimal convergence rate. We also show that linear distribution estimates, such as the empirical distribution or kernel density estimator, often fail to converge at the optimal rate. Our bounds generalize, unify, or improve several recent and classical results. Moreover, IPMs can be used to formalize a statistical model of generative adversarial networks (GANs). Thus, we show how our results imply bounds on the statistical error of a GAN, showing, for example, that GANs can strictly outperform the best linear estimator.
